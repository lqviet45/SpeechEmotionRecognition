import streamlit as st
import librosa
import numpy as np
import joblib
import os
import io
from audiorecorder import audiorecorder
from scipy.signal import butter, lfilter

# --- C·∫•u h√¨nh Trang (FE) ---
st.set_page_config(page_title="Nh·∫≠n d·∫°ng C·∫£m x√∫c", layout="wide")
st.title("üé§ ·ª®ng d·ª•ng Nh·∫≠n d·∫°ng C·∫£m x√∫c Gi·ªçng n√≥i")

# --- H√ÄM L·ªåC (BE) ---
def butter_bandpass_filter(data, lowcut=100.0, highcut=8000.0, fs=22050, order=5):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    y_filtered = lfilter(b, a, data)
    return y_filtered

# --- T·∫£i M√¥ h√¨nh (BE) - V2 ---
MODEL_PATH = os.path.join(os.path.dirname(__file__), '..', 'models')
try:
    model = joblib.load(os.path.join(MODEL_PATH, "rf_emotion_model_v2.pkl"))
    scaler = joblib.load(os.path.join(MODEL_PATH, "scaler_v2.pkl"))
    encoder = joblib.load(os.path.join(MODEL_PATH, "encoder_v2.pkl"))
    st.sidebar.success("T·∫£i m√¥ h√¨nh (RF v2), Scaler (v2), v√† Encoder (v2) th√†nh c√¥ng!")
except Exception as e:
    st.error(f"L·ªói khi t·∫£i m√¥ h√¨nh v2: {e}")
    st.stop()

# --- H√†m Tr√≠ch xu·∫•t ƒê·∫∑c tr∆∞ng (BE) - (ƒê√É S·ª¨A L·ªñI libS) ---
def extract_features(y, sr=22050):
    try:
        y_filtered = butter_bandpass_filter(y, fs=sr)
        mfccs = np.mean(librosa.feature.mfcc(y=y_filtered, sr=sr, n_mfcc=20).T, axis=0)
        
        rms = np.mean(librosa.feature.rms(y=y_filtered).T, axis=0) 
        
        zcr = np.mean(librosa.feature.zero_crossing_rate(y=y_filtered).T, axis=0)
        features = np.hstack((mfccs, rms, zcr))
        return features
    except Exception as e:
        st.error(f"L·ªói khi tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng: {e}")
        return None

# --- H√†m X·ª≠ l√Ω v√† D·ª± ƒëo√°n (BE) ---
def process_and_predict(y, sr):
    with st.spinner("ƒêang √°p d·ª•ng b·ªô l·ªçc (DSP), tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng v√† ch·∫°y m√¥ h√¨nh AI..."):
        features = extract_features(y, sr)
        
        if features is not None:
            features_2d = features.reshape(1, -1)
            features_scaled = scaler.transform(features_2d)
            
            prediction_encoded = model.predict(features_scaled)
            prediction_label = encoder.inverse_transform(prediction_encoded)[0]
            
            st.subheader("K·∫øt qu·∫£ Ph√¢n lo·∫°i:")
            prediction_label_str = str(prediction_label).replace("np.str_('", "").replace("')", "")
            st.success(f"C·∫£m x√∫c ƒë∆∞·ª£c d·ª± ƒëo√°n l√†: **{prediction_label_str.capitalize()}**")

# --- Kh·ªüi t·∫°o Session State (Quan tr·ªçng) ---
if 'last_processed_audio' not in st.session_state:
    st.session_state.last_processed_audio = None

# --- Giao di·ªán (FE) d√πng Tabs ---
tab1, tab2 = st.tabs(["üìÅ T·∫£i file l√™n", "üéôÔ∏è Thu √¢m tr·ª±c ti·∫øp"])

# ----- Tab 1: T·∫£i file l√™n -----
with tab1:
    st.header("Ph∆∞∆°ng th·ª©c 1: T·∫£i file √¢m thanh (.wav, .mp3)")
    uploaded_file = st.file_uploader("Ch·ªçn file √¢m thanh...", type=["wav", "mp3", "ogg"], key="file_uploader")

    if uploaded_file is not None:
        st.subheader("File √¢m thanh b·∫°n ƒë√£ t·∫£i l√™n:")
        st.audio(uploaded_file)
        
        y, sr = librosa.load(uploaded_file, sr=22050)
        process_and_predict(y, sr)

# ----- Tab 2: Thu √¢m tr·ª±c ti·∫øp (ƒê√É S·ª¨A L·ªñI LOGIC) -----
with tab2:
    st.header("Ph∆∞∆°ng th·ª©c 2: Thu √¢m gi·ªçng n√≥i c·ªßa b·∫°n")
    st.write("B·∫•m n√∫t b√™n d∆∞·ªõi, n√≥i, sau ƒë√≥ b·∫•m d·ª´ng. App s·∫Ω ph√¢n t√≠ch sau khi b·∫°n b·∫•m d·ª´ng.")

    audio_segment = audiorecorder(
        start_prompt="B·∫•m ƒë·ªÉ b·∫Øt ƒë·∫ßu ghi √¢m ‚è∫Ô∏è",
        stop_prompt="B·∫•m ƒë·ªÉ d·ª´ng ghi √¢m ‚èπÔ∏è",
        pause_prompt="",
    )

    if audio_segment:
        # KI·ªÇM TRA XEM ƒê√ÇY C√ì PH·∫¢I B·∫¢N THU √ÇM M·ªöI KH√îNG
        if audio_segment != st.session_state.last_processed_audio:
            
            # 1. ƒê√°nh d·∫•u l√† ƒë√£ x·ª≠ l√Ω
            st.session_state.last_processed_audio = audio_segment
            
            st.subheader("B·∫£n thu √¢m m·ªõi nh·∫≠n ƒë∆∞·ª£c:")
            
            # 2. Chuy·ªÉn ƒë·ªïi AudioSegment -> bytes
            wav_buffer = io.BytesIO()
            audio_segment.export(wav_buffer, format="wav")
            wav_bytes = wav_buffer.getvalue()

            # 3. Ph√°t √¢m thanh
            st.audio(wav_bytes, format="audio/wav")
            
            # 4. T·∫£i v√†o Librosa
            audio_file_like = io.BytesIO(wav_bytes)
            y, sr = librosa.load(audio_file_like, sr=22050)
            
            # 5. G·ªçi h√†m x·ª≠ l√Ω (CH·ªà CH·∫†Y 1 L·∫¶N)
            process_and_predict(y, sr)
        
        # N·∫øu audio_segment gi·ªëng h·ªát l·∫ßn tr∆∞·ªõc (do rerun),
        # code s·∫Ω kh√¥ng ch·∫°y v√†o 'if' n√†y v√† kh√¥ng d·ª± ƒëo√°n l·∫°i.
